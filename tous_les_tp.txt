je vais te donner absolument tous les codes qu'on m'a donné (9 tp différents)

!pip install ipywidgets jupyter langchain-core langchain-community langchain-google-genai langchain-openai langchain-experimental

import os
from langchain_google_genai import ChatGoogleGenerativeAI

# Déclaration de la clef d'API
os.environ["GOOGLE_API_KEY"] = "MA_CLE_GOOGLE_ICI"

# Création d'une instance de chat
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3, max_tokens=None)

# Autres modèles
# llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3, max_tokens=None)
# llm = init_chat_model("mistral-large-latest", model_provider="mistralai")
# llm = init_chat_model("claude-3-7-sonnet-20250219", model_provider="anthropic")
# llm = init_chat_model("deepseek-chat", model_provider="deepseek")
# ... plus de modèles ici : https://lmarena.ai/leaderboard

response = llm.invoke("Hello Gemini, how are you?")
print(response.content)


# Prompt de génération narrative
prompt_histoire = """
Tu es un narrateur talentueux. Génère une micro-nouvelle (3-4 phrases) qui inclut les éléments suivants :
* Personnage : Un {personnage}
* Lieu : {lieu}
* Thème : {theme}
Ton style doit être {style}.
"""

# Remplissez les variables du prompt ci-dessous
input_dict = {
    "personnage": "maxime",
    "lieu": "bayonne",
    "theme": "l'espoir face à l'inconnu",
    "style": "poétique et mélancolique"
}

# Formatage et envoi du prompt
histoire_finale = llm.invoke(prompt_histoire.format(**input_dict))

print("Génération narrative")
print(histoire_finale.content)


from langchain.prompts import ChatPromptTemplate

print("Recommande des emplois pour une personne agée. Sois concis")
messages = ChatPromptTemplate.from_template(
    "Recommande des emplois pour une personne agée. Sois concis."
).format_prompt().to_messages()
response = llm(messages)
print(f"{response.content}")


print("Recommande des emplois pour une personne jeune. Sois concis.")
messages = ChatPromptTemplate.from_template(
    "Recommande des emplois pour une personne jeune. Sois concis."
).format_prompt().to_messages()
response = llm(messages)
print(f"{response.content}")


TP2 
from langchain.prompts import PromptTemplate

# Définition du template avec des variables entre accolades {}
template = """
Tu es un expert en {domaine}.
Concevez un argument de vente unique et convaincant pour le produit suivant : {produit}.
L'argument doit être adapté à une cible {cible} et ne pas dépasser 3 phrases.
"""

# Création de l'objet PromptTemplate
prompt_template = PromptTemplate(
    input_variables=["domaine", "produit", "cible"],  # Liste des variables à remplir
    template=template,
)

# Formatage du prompt avec des valeurs concrètes
prompt_formate = prompt_template.format(
    domaine="technologie wearable",
    produit="une montre intelligente qui analyse le stress",
    cible="des dirigeants d'entreprise très occupés"
)

print("Prompt formatté")
print(prompt_formate)

# Envoi du prompt formaté au LLM
reponse = llm.invoke(prompt_formate)
print("Réponse du LLM")
print(reponse.content)


from langchain.prompts import ChatPromptTemplate
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Définition des templates pour chaque rôle
system_template = PromptTemplate(
    input_variables=['style'],
    template="Tu es un assistant poète expert. Tu écris toujours tes réponses dans un style {style}."
)
human_template = PromptTemplate(
    input_variables=['sujet'],
    template="Compose un haïku (3 lignes, 5-7-5 syllabes) sur le thème de : {sujet}."
)

# Création des templates de messages
system_message_prompt = SystemMessagePromptTemplate(prompt=system_template)
human_message_prompt = HumanMessagePromptTemplate(prompt=human_template)

# Combinaison des messages en un seul ChatPromptTemplate
chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])

# Formatage final avec toutes les variables
messages_formates = chat_prompt.format_prompt(
    style="japonais traditionnel et contemplatif",
    sujet="le passage du temps"
).to_messages()

print("Messages formattés pour le chat")
for msg in messages_formates:
    print(f"{msg.type}: {msg.content}\n")

# Envoi des messages au LLM
reponse = llm.invoke(messages_formates)

print("Texte généré")
print(reponse.content)


from langchain.prompts import ChatPromptTemplate
from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate

# Définition du template pour le rôle
system_template = PromptTemplate(
    input_variables=['style'],
    template="Tu es un assistant poète expert. Tu écris toujours tes réponses dans un style {style}."
)
template_biographie = PromptTemplate(
    input_variables=['personnage','metier','univers'],
    template="Écris une biographie très courte et concise de {personnage}, une {metier} dans l'univers de {univers}."
)

# Création des templates des messages
system_message_prompt = SystemMessagePromptTemplate(prompt=system_template)
biographie_message_prompt = HumanMessagePromptTemplate(prompt=template_biographie)

# Combinaison du message en un seul ChatPromptTemplate
chat_prompt = ChatPromptTemplate.from_messages([biographie_message_prompt],[system_message_prompt])

messages_formates = chat_prompt.format_prompt(
    personnage="Elara",
    metier="forgeronne elfique",
    univers="Donjons & Dragons",
    style="japonais traditionnel et contemplatif"
).to_messages()

print("Messages formattés pour le chat")
for msg in messages_formates:
    print(f"{msg.type}: {msg.content}\n")

reponse = llm.invoke(messages_formates)
print("Texte généré")
print(reponse.content)

TP3
rom langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SimpleSequentialChain, SequentialChain

# Définition du template et création de la prompt template
template_avis = """
Tu es un critique gastronomique. Donne un avis court et percutant sur le restaurant suivant :
Restaurant : {nom_restaurant}
Cuisine : {type_cuisine}
Avis :
"""
prompt_avis = PromptTemplate(
    input_variables=["nom_restaurant", "type_cuisine"],
    template=template_avis
)

# Création de la LLMChain
chain_avis = LLMChain(llm=llm, prompt=prompt_avis, verbose=True)

# Exécution de la chaine
resultat = chain_avis.invoke({
    "nom_restaurant": "Le Petit Jardin",
    "type_cuisine": "française moderne"
})

print("Résultat de la chaîne")
print(resultat['text'])


from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.chains import SimpleSequentialChain

# Chaîne 1 : Résumé
prompt_resume = PromptTemplate(
    input_variables=["texte"],
    template="""
Résumez le texte suivant en une seule phrase concise :
Texte : {texte}
Résumé :
"""
)

# Chaîne 2 : Traduction
prompt_traduction = PromptTemplate(
    input_variables=["resume"],
    template="""
Traduisez le texte suivant en anglais :
Texte : {resume}
Traduction :
"""
)

chain_resume = LLMChain(llm=llm, prompt=prompt_resume, output_key="resume", verbose=True)
chain_traduction = LLMChain(llm=llm, prompt=prompt_traduction, output_key="traduction_anglaise", verbose=True)

# Chaînage avec l'opérateur '|'
overall_chain = chain_resume | chain_traduction

# Même chose que..
# overall_chain = SimpleSequentialChain(
#    chains=[chain_resume, chain_traduction],
#    verbose=True
#)

# Texte à traiter
texte_a_traiter = """
L'apprentissage automatique est un domaine de l'intelligence artificielle qui permet aux systèmes d'apprendre et de s'améliorer à partir de données sans être explicitement programmés pour chaque tâche. Il se divise en plusieurs types, notamment l'apprentissage supervisé, non supervisé et par renforcement.
"""

# Exécution de la chaîne
resultat_final = overall_chain.invoke({"texte": texte_a_traiter})

print("\nTexte original")
print(texte_a_traiter)
print("\nTexte résumé")
print(resultat_final['resume'])
print("\nTexte traduit")
print(resultat_final['traduction_anglaise'])


from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

# Chaîne d'analyse de sentiment
chain_sentiment = LLMChain(
    llm=llm,
    prompt=PromptTemplate(
        input_variables=["texte"],
        template="""
Analyse le sentiment du texte suivant. Réponds uniquement par un seul mot : Positif, Negatif ou Neutre.
Texte : {texte}
Analyse :
"""
    ),
    output_key="analyse_sentiment"
)

# Chaînage avec l'opérateur '|'
pipeline_avance = chain_resume | chain_traduction | chain_sentiment

# Texte à analyser
texte_avec_sentiment = """
J'ai récemment acheté ce produit et je suis extrêmement déçu. La qualité est médiocre, il s'est cassé après deux jours d'utilisation et le service client a été incapable de résoudre mon problème. Je déconseille fortement.
"""

# Exécution de la chaîne
resultat_avance = pipeline_avance.invoke({"texte": texte_avec_sentiment})

# Affichage des résultats
print("\nTexte original")
print(texte_avec_sentiment)
print("\nTexte résumé")
print(resultat_avance['resume'])
print("\nTexte traduit")
print(resultat_avance['traduction_anglaise'])
print("\nSentiment du texte")
print(resultat_avance['analyse_sentiment'])



def pipeline_conditionnel(texte, longueur_seuil=100):
    """
    Pipeline intelligent : résume si le texte est long, analyse directement sinon.
    """
    print(f"Longueur du texte : {len(texte)} caractères (seuil = {longueur_seuil})")

    # Condition : Si le texte est long, on le résume d'abord
    if len(texte) > longueur_seuil:
        print("Texte long détecté. Déclenchement du résumé...")
        resume = chain_resume.run(texte)
        texte_pour_analyse = resume
        print(f"Texte résumé pour analyse : '{texte_pour_analyse}'")
    else:
        print("Texte court. Analyse directe...")
        texte_pour_analyse = texte

    # Analyse de sentiment sur le texte original ou résumé
    analyse = chain_sentiment.invoke(texte_pour_analyse)
    return analyse


print("TEST AVEC TEXTE LONG")
texte_long = "L'impact de l'intelligence artificielle sur la société moderne est un sujet de débat intense et complexe. D'un côté, ses partisans mettent en avant les progrès médicaux spectaculaires, l'optimisation des processus industriels qui réduit l'impact environnemental, et la création de nouveaux métiers. De l'autre, les détracteurs pointent des risques importants : l'automatisation massive pourrait supprimer des millions d'emplois sans création suffisante de nouveaux postes, les biais algorithmiques pourraient perpétuer et même amplifier les discriminations existantes, et la concentration du pouvoir technologique entre les mains de quelques grandes entreprises pose des problèmes éthiques et démocratiques. Trouver un équilibre entre innovation et régulation sera l'un des défis majeurs des prochaines décennies."
resultat_conditionnel_long = pipeline_conditionnel(texte_long, longueur_seuil=100)
print(f"Résultat de l'analyse de sentiment : {resultat_conditionnel_long}")

print("\n" + "="*50 + "\n")

print("TEST AVEC TEXTE COURT")
texte_court = "Ce produit est tout simplement génial !"
resultat_conditionnel_court = pipeline_conditionnel(texte_court, longueur_seuil=100)
print(f"Résultat de l'analyse de sentiment : {resultat_conditionnel_court}")


from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key="history", return_messages=True)
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

print(conversation.run("Salut ! Qui a écrit Le Petit Prince ?"))
# → Réponse : "Antoine de Saint-Exupéry."

print(conversation.run("Quel âge avait-il quand il l'a publié ?"))
# → Réponse : "Il avait 44 ans lorsqu'il a publié Le Petit Prince en 1943."

print(conversation.run("Et quel est le thème principal de ce livre ?"))
# → Réponse : "Le thème principal est la découverte de l'amitié, de l'amour et de la simplicité de la vie vue à travers les yeux d'un enfant."


import os
from langchain import LLMChain
from langchain_experimental.sql import SQLDatabaseChain
from langchain.sql_database import SQLDatabase

# Télécharger la base de données NORTHWIND au format SQLite
!wget https://raw.githubusercontent.com/jpwhite3/northwind-SQLite3/main/dist/northwind.db

# Connecter la base de données SQLite
db = SQLDatabase.from_uri("sqlite:///northwind.db")

# Créer la chaîne
db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)

# Poser une question
question = "Quels sont les 5 produits avec le prix unitaire le plus élevé ?"
answer = db_chain.run(question)
print("Réponse :", answer)



from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_community.utilities import SQLDatabase
from langchain_experimental.sql import SQLDatabaseChain
from langchain.chains.router import MultiRouteChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

# ===== 1. Création des chaînes de base =====

# SQLDatabaseChain pour questions sur la BDD (attend un paramètre 'query')
db = SQLDatabase.from_uri("sqlite:///northwind.db")
base_sql_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)

# LLMChain pour questions générales (attend un paramètre 'question')
general_prompt = PromptTemplate(
    input_variables=["question"],
    template="Réponds à cette question de manière claire : {question}"
)
base_general_chain = LLMChain(llm=llm, prompt=general_prompt)

# ===== 2. Création des wrappers pour uniformiser l'interface =====
# MultiRouteChain passe toujours un dict avec la clé 'input'
# Il faut donc wrapper les chaînes pour convertir 'input' vers leur clé attendue

from langchain_core.runnables import RunnableLambda
from langchain.chains.base import Chain

class SQLChainWrapper(Chain):
    """Wrapper pour SQLDatabaseChain qui convertit 'input' en 'query'"""

    sql_chain: Chain

    @property
    def input_keys(self):
        return ["input"]

    @property
    def output_keys(self):
        return ["text"]

    def _call(self, inputs):
        # Convertir 'input' en 'query' pour SQLDatabaseChain
        result = self.sql_chain.invoke({"query": inputs["input"]})
        return {"text": result["result"]}

class GeneralChainWrapper(Chain):
    """Wrapper pour LLMChain qui convertit 'input' en 'question'"""

    general_chain: Chain

    @property
    def input_keys(self):
        return ["input"]

    @property
    def output_keys(self):
        return ["text"]

    def _call(self, inputs):
        # Convertir 'input' en 'question' pour LLMChain
        result = self.general_chain.invoke({"question": inputs["input"]})
        return {"text": result["text"]}

# Créer les chaînes wrappées
sql_chain = SQLChainWrapper(sql_chain=base_sql_chain)
general_chain = GeneralChainWrapper(general_chain=base_general_chain)

# ===== 3. Définition des descriptions des chaînes =====

prompt_infos = [
    {
        "name": "sql",
        "description": "Bon pour répondre aux questions sur les données de la base de données Northwind, "
                      "les produits, les clients, les commandes, les prix, les quantités, etc.",
    },
    {
        "name": "general",
        "description": "Bon pour répondre aux questions générales, culture générale, "
                      "connaissances générales, et tout ce qui ne nécessite pas de base de données",
    }
]

# ===== 4. Création du dictionnaire de chaînes de destination =====

destination_chains = {
    "sql": sql_chain,
    "general": general_chain
}

# ===== 5. Création du RouterChain avec LLM =====

# Construction du template de routage
destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
destinations_str = "\n".join(destinations)

# Template de routage personnalisé
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations_str
)

# Prompt du router
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)

# Création du LLMRouterChain
router_chain = LLMRouterChain.from_llm(llm, router_prompt, verbose=True)

# ===== 6. Création du MultiRouteChain =====

# Chaîne par défaut si aucune destination n'est appropriée
default_chain = general_chain

# Création de la chaîne multi-route complète
multi_route_chain = MultiRouteChain(
    router_chain=router_chain,
    destination_chains=destination_chains,
    default_chain=default_chain,
    verbose=True
)

# ===== 7. Tests =====

print("="*60)
print("TESTS AVEC MultiRouteChain")
print("="*60)

question1 = "Quels sont les 5 produits les plus chers ?"
question2 = "Qui a écrit Le Petit Prince ?"

print("\n" + "="*60)
print("Question 1:", question1)
print("="*60)
result1 = multi_route_chain.invoke({"input": question1})
print("\nRéponse:", result1)

print("\n" + "="*60)
print("Question 2:", question2)
print("="*60)
result2 = multi_route_chain.invoke({"input": question2})
print("\nRéponse:", result2)

print("\n" + "="*60)
print("Routage automatique effectué par LangChain !")
print("="*60)


TP4
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.output_parsers import CommaSeparatedListOutputParser, StructuredOutputParser
from langchain.output_parsers import ResponseSchema, StructuredOutputParser
from langchain.schema.output_parser import StrOutputParser
from langchain.chains import LLMChain

# Initialisation du Parser
list_parser = CommaSeparatedListOutputParser()

# Récupération du format d'instructions depuis le parser
format_instructions = list_parser.get_format_instructions()
print("Format Instructions:")
print(format_instructions)

# Création du template en intégrant les instructions de format
template = """
Liste les trois principales technologies derrière le Web moderne.
Réponds uniquement avec une liste de mots, sans numérotation, ponctuation superflue ou texte supplémentaire.
{format_instructions}
"""
prompt = PromptTemplate(
    template=template,
    input_variables=[],
    partial_variables={"format_instructions": format_instructions}
)

# Création et exécution de la chaine
# chain = prompt | llm | list_parser # Syntaxe pipe (|) très concise !

# Autre syntaxe
chain_verbose = LLMChain(
    llm=llm,
    prompt=prompt,
    output_parser=list_parser,
    verbose=True
)

resultat = chain_verbose.invoke({})

print("\n=== RÉSULTAT (Type: {}) ===".format(type(resultat)))
print(resultat)


# Définition du schéma de la réponse que l'on veut obtenir
schemas = [
    ResponseSchema(name="resume", description="Un résumé très concis de l'article en une phrase."),
    ResponseSchema(name="mots_cles", description="Une liste de 3 à 5 mots-clés pertinents pour cet article."),
    ResponseSchema(name="sentiment", description="Une estimation du sentiment global de l'article. Choix: Positif, Negatif, Neutre."),
    ResponseSchema(name="score_pertinence", description="Un score de pertinence sur 10 pour un public étudiant."),
    ResponseSchema(name="public_cible", description="Le public principal qui serait intéressé par cet article.")
]

# Initialisation du parser avec nos schémas
structured_parser = StructuredOutputParser.from_response_schemas(schemas)

# Récupération des instructions de formatage
format_instructions = structured_parser.get_format_instructions()
print("Format Instructions pour le parser structuré:")
print(format_instructions)

# Création du template en intégrant les instructions
template = """
Analyse l'article suivant selon les instructions.

Article:
{article}

{format_instructions}
"""
prompt = ChatPromptTemplate.from_template(template)

# Création de la chaine complète
chain = prompt | llm | structured_parser

# Test avec un article
article_test = """
L'intelligence artificielle generative, comme GPT-4, révolutionne l'éducation. Des outils permettent désormais de créer des exercices personnalisés, d'expliquer des concepts complexes de différentes manières et même de corriger des devoirs en donnant un feedback constructif. Cela pourrait aider à réduire les inégalités d'accès au tutorat personnalisé. Cependant, des défis persistent, comme le risque de hallucinations ou de biais dans les réponses générées, nécessitant une vigilance constante des éducateurs.
"""

resultat_structure = chain_verbose.invoke({"article": article_test, "format_instructions": format_instructions})

print("\nRésultat structuré")
print(f"Type de l'objet : {type(resultat_structure)}")
print(f"Clés disponibles : {list(resultat_structure.keys())}")
print("\nDétail:")
for key, value in resultat_structure.items():
    print(f"- {key}: {value} (Type: {type(value)})")


# Définir le schéma de ce que l'on veut extraire
schemas_extraction = [
    ResponseSchema(name="personnage_principal", description="Le nom du personnage principal. Retourne 'Inconnu' si non spécifié."),
    ResponseSchema(name="lieu_principal", description="Le lieu où se déroule l'action principale."),
    ResponseSchema(name="conflit_principal", description="Le problème ou le conflit central de l'histoire en une phrase."),
    ResponseSchema(name="genre", description="Le genre littéraire de l'histoire. Ex: Science-Fiction, Romance, Horreur, etc."),
    ResponseSchema(name="editeur", description="L'éditeur du livre contenant cette histoire'.")

]

# Initialisation du parser et récupération des instructions
parser_extraction = StructuredOutputParser.from_response_schemas(schemas_extraction)
format_instructions_extraction = parser_extraction.get_format_instructions()

# Création du template avec instruction de format
template_extraction = """
Extrais les informations demandées de l'histoire suivante.

Histoire:
{histoire}

{format_instructions}
"""
prompt_extraction = ChatPromptTemplate.from_template(template_extraction)

# Construction et exécution de la chaine
chain_extraction = prompt_extraction | llm | parser_extraction

# Histoire à analyser
histoire = """
Le docteur Aïcha Mendes ajusta ses lunettes connectées dans le laboratoire sous-marin Néréide. Les capteurs indiquaient une activité sismique inhabituelle près de la fosse des Mariannes. Si elle et son équipe n’arrivaient pas à stabiliser les générateurs de pression, la station risquait d’être écrasée par la montée brutale des courants. Le temps jouait contre eux : les réserves d’énergie ne dureraient que quelques heures."""

resultat_extraction = chain_extraction.invoke({"histoire": histoire, "format_instructions": format_instructions_extraction})

print("=== INFORMATIONS EXTRACTIVES ===")
for key, value in resultat_extraction.items():
    print(f"{key.replace('_', ' ').title()}: {value}")

TP5
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryMemory
from langchain.chains import ConversationChain
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder

# Initialisation de la mémoire avec return_messages=True
memory = ConversationBufferMemory(return_messages=True)

# Setup de la chaîne conversationnelle
prompt = ChatPromptTemplate.from_messages([
    ("system", "Tu es un assistant conversationnel utile."),
    MessagesPlaceholder(variable_name="history"),
    ("human", "{input}")
])

# Méthode avec ConversationChain
chain = ConversationChain(
    llm=llm,
    prompt=prompt,
    memory=memory,
    verbose=True
)

# Test de conversation
inputs = [
    "Bonjour, je m'appelle Jean",
    "Quel temps fait-il aujourd'hui ?",
    "Qu'est-ce que je viens de te dire sur moi ?"
]

for user_input in inputs:
    response = chain.invoke({"input": user_input})
    print(f"Utilisateur: {user_input}")
    print(f"Assistant: {response['response']}")
    print(f"Mémoire: {memory.load_memory_variables({})['history']}\n")


from langchain.memory import ConversationBufferWindowMemory

window_memory = ConversationBufferWindowMemory(k=5, return_messages=True)

window_chain = ConversationChain(
    llm=llm,
    memory=window_memory,
    verbose=False
)

print("=== Test ConversationBufferWindowMemory ===")
test_inputs = ["Bonjour, je suis Jean", "Comment je m'appelle ?", "Il fait beau aujourd'hui", "La température est de 22 degrée", "Comment je m'appelle ?"]
for user_input in test_inputs:
    response = window_chain.invoke({"input": user_input})
    print(response["response"])
    current_memory = window_memory.load_memory_variables({})['history']
    print(f"Mémoire après '{user_input}': {len(current_memory)} messages")


from langchain.memory import ConversationSummaryMemory

summary_memory = ConversationSummaryMemory(
    llm=llm,
    return_messages=True
)

summary_chain = ConversationChain(
    llm=llm,
    memory=summary_memory,
    verbose=True
)

print("=== Test ConversationSummaryMemory ===")
long_conversation = [
    "J'aime la pizza margherita",
    "Je préfère les pâtes carbonara",
    "Mon film préféré est Le Parrain",
    "Je voyage en Italie la semaine prochaine"
]

for user_input in long_conversation:
    response = summary_chain.invoke({"input": user_input})
    memory_content = summary_memory.load_memory_variables({})['history']
    print(f"Résumé actuel: {str(memory_content)}...\n")


TP6

from langchain.agents import AgentType, initialize_agent, Tool, load_tools
from langchain.schema import SystemMessage

# Chargement des outils
tools = load_tools(["llm-math"], llm=llm)

# Inspection des outils chargés
print("Outils disponibles :")
for i, tool in enumerate(tools):
    print(f"{i+1}. {tool.name}: {tool.description}")

# Création de l'agent avec la stratégie ReAct
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,  # Type d'agent pour le chat
    verbose=True,  # Essentiel pour voir le processus ReAct !
    handle_parsing_errors=True  # Gère les erreurs de parsing

)

# Test de l'agent avec un problème simple
print("TEST DE L'AGENT AVEC OUTILS:")
question = "Quelle est le résultat de 8*5"
resultat = agent.invoke(question)
print(f"\nRÉPONSE FINALE: {resultat['output']}")

from langchain.agents import tool
from datetime import datetime
from langchain.utilities import WikipediaAPIWrapper
import math
import re

# Définition d'un outil personnalisé avec le décorateur @tool
# Cette syntaxe permet de créer rapidement un outil à partir d'une fonction Python

# Outil 1
@tool
def get_current_time_tool(format: str = "%H:%M:%S") -> str:
    """Retourne l'heure actuelle.
    Utile pour répondre aux questions sur l'heure ou la date.
    Prend en paramètre le format attendu par strftime"""
    return datetime.now().strftime(format)


# Définition de deux outils personnalisés via l'objet Tool
# Cette syntaxe permet de spécifier explicitement le nom, la fonction et la description de l'outil

# Outil 2
wikipedia_tool = Tool(
    name="wikipedia",
    func=WikipediaAPIWrapper().run,
    description="Permet de rechercher et obtenir des informations depuis Wikipedia."
)

def safe_calculator(expression):
    """
    Utilisez pour effectuer des calculs mathématiques. Entrée: une expression mathématique comme '2 + 2' ou 'math.sqrt(16)
    """
    # Nettoyer l'expression pour éviter l'injection de code
    expression = re.sub(r'[^0-9+\-*/().]', '', expression)

    try:
        result = eval(expression, {"__builtins__": None}, {"math": math})
        return f"Résultat: {result}"
    except Exception as e:
        return f"Erreur: {str(e)}"

# Outil 3
calculator_tool = Tool(
    name="Calculator",
    func=safe_calculator,
    description="Utilisez pour effectuer des calculs mathématiques. Entrée: une expression mathématique comme '2 + 2' ou 'math.sqrt(16)'"
)

# Ajout des outils personnalisé à notre liste
tools_avances = [calculator_tool, get_current_time_tool, wikipedia_tool]

print("Outils disponibles :")
for i, tool in enumerate(tools_avances):
    print(f"{i+1}. {tool}")


# Création d'un agent plus sophistiqué
agent_avance = initialize_agent(
    tools=tools_avances,
    llm=llm,
    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True
)

# Test avec une question nécessitant plusieurs outils
print("Test avec outils personalisés:")
questions_complexes = [
    "Quelle heure est-il en ce moment ?",
    "Quel est la population de Paris au carré",
    "Si la population de Paris est supérieur à 1,000,000,000 donne moi l'heure actuel, sinon refuse de me répondre.",
    "Qui a inventé le langage Python et quelle est la population de son pays ?"
]

for question in questions_complexes:
    print(f"\nQUESTION: {question}")
    resultat = agent_avance.invoke(question)
    print(f"\nRÉPONSE: {resultat['output']}")
    print("-" * 50)


TP7 

!pip install ipywidgets jupyter langchain-core langchain-community langchain-google-genai langchain-openai langchain_experimental
!pip install wikipedia numexpr requests sqlalchemy duckdb geopy tavily-python bs4 gTTS
!wget https://raw.githubusercontent.com/jpwhite3/northwind-SQLite3/main/dist/northwind.db

import os
from langchain_google_genai import ChatGoogleGenerativeAI

# Déclaration de la clef d'API
os.environ["GOOGLE_API_KEY"] = "AIzaSyDra8dzulksCHQhjiaU6v-Mqp4yUR4dg6E"
os.environ["TAVILY_API_KEY"] = "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"

# Création d'une instance de chat
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.3, max_tokens=None)

from langchain.agents import AgentType, initialize_agent, Tool
from langchain_experimental.tools import PythonREPLTool
from langchain_community.utilities import SQLDatabase
from langchain_community.agent_toolkits import SQLDatabaseToolkit
import requests
import json
import sqlite3
from typing import Dict, Any

class OSMGeoTools:
    @staticmethod
    def geocode_address(address: str) -> str:
        try:
            url = f"https://nominatim.openstreetmap.org/search?q={address}&format=json&limit=1"
            headers = {'User-Agent': 'LangChain-Agent-TP'}
            response = requests.get(url, headers=headers)

            if response.status_code == 200:
                data = response.json()
                if data:
                    result = data[0]
                    return f"{address} → Lat: {result['lat']}, Lon: {result['lon']}"
                else:
                    return f"Adresse non trouvée: {address}"
            else:
                return f"Erreur API: {response.status_code}"

        except Exception as e:
            return f"Erreur: {str(e)}"

    @staticmethod
    def find_poi_nearby(queryString: str) -> str:
        parts = queryString.split(",")
        if len(parts) != 4:
          return "Format attendu: lat,lon,type,rayon"
        lat, lon, poi_type, radius = parts

        try:
            url = f"https://overpass-api.de/api/interpreter"
            query = f"""
            [out:json];
            (
              node["amenity"="{poi_type}"](around:{radius},{lat},{lon});
              way["amenity"="{poi_type}"](around:{radius},{lat},{lon});
              relation["amenity"="{poi_type}"](around:{radius},{lat},{lon});
            );
            out center;
            """
            response = requests.post(url, data=query)

            if response.status_code == 200:
                data = response.json()
                pois = []
                for element in data['elements']:
                  pois.append(element)

                return f"{poi_type} à proximité ({radius}m):\n" + "\n".join(
                    str(poi) for poi in pois[:10]
                )
            else:
                return f"Erreur API: {response.status_code}"

        except Exception as e:
            return f"Erreur: {str(e)}"

osm_tools = [
    Tool(
        name="Geocode Tool",
        description="Convertit une adresse en coordonnées latitude/longitude. Input: une adresse complète",
        func=OSMGeoTools.geocode_address
    ),
    Tool(
        name="Find POI Tool",
        description="Trouve des points d'intérêt près de coordonnées. Input: une chaine de type 'lat,lon,type_poi,rayon'. type is an osm amenity type like restaurant, school, hospital, etc.",
        func=OSMGeoTools.find_poi_nearby
    )
]

print("Outils OSM créés!")
print(OSMGeoTools.geocode_address("Tour Eiffel, Paris"))
print(OSMGeoTools.find_poi_nearby("48.8611473,2.3380277,restaurant,500"))


from langchain_community.tools.tavily_search import TavilySearchResults
from langchain.agents import initialize_agent, Tool, AgentType
from langchain.agents import AgentExecutor
import requests
from bs4 import BeautifulSoup

class SearchTool:
    def __init__(self, max_results=5):
        self.search_tool = TavilySearchResults(max_results=max_results)

    def run(self, query):
        return self.search_tool.run(query)

search_tool = SearchTool(max_results=5)

class ReadPageTool:
    def __init__(self):
        pass

    def run(self, url):
        return self.read_page_content(url)

    def read_page_content(self, url):
        try:
            response = requests.get(url)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')
            page_text = soup.get_text()
            return page_text.strip()
        except requests.exceptions.RequestException as e:
            return f"Error fetching the page: {e}"

read_page_tool = ReadPageTool()

web_tools = [
    Tool(
        name="Search Tool",
        func=search_tool.run,
        description="Searches the web for a given query and returns a list of results."
    ),
    Tool(
        name="Read Page Tool",
        func=read_page_tool.run,
        description="Reads the content of a given webpage URL and returns the text."
    )
]


print("Test")

search_results = search_tool.run("Latest research on quantum computing 2025")
print(f"Search Results: {search_results}\n")

if search_results:
    first_result = search_results[0]
    url = first_result['url']
    print(f"Testing Read Page Tool with URL: {url}")
    page_content = read_page_tool.run(url)
    print(f"Page Content (first 500 characters): {page_content[:500]}...")
else:
    print("No search results found!")


import sqlite3

def quote_identifier(identifier: str) -> str:
    return f'"{identifier.replace("\"", "\"\"")}"'

class GenericSQLTool:
    def __init__(self, db_path: str = "./northwind.db"):
        self.db_path = db_path
        self.connection = sqlite3.connect(db_path)
        self.connection.row_factory = sqlite3.Row  # Pour avoir des résultats sous forme de dict

    def get_table_schema(self, table_name: str = None) -> str:
        try:
            cursor = self.connection.cursor()

            if table_name:
                # Schéma d'une table spécifique
                cursor.execute(f"PRAGMA table_info({quote_identifier(table_name)})")
                columns = cursor.fetchall()
                schema = f"Schéma de la table '{table_name}':\n"
                for col in columns:
                    schema += f"- {col['name']} ({col['type']})\n"
                return schema
            else:
                # Liste toutes les tables
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                schema = "Tables disponibles dans la base:\n"
                for table in tables:
                    schema += f"\n{table['name']}:\n"
                    cursor.execute(f"PRAGMA table_info({quote_identifier(table['name'])})")
                    columns = cursor.fetchall()
                    for col in columns:
                        schema += f"  - {col['name']} ({col['type']})\n"
                return schema

        except Exception as e:
            return f"Erreur lors de la récupération du schéma: {str(e)}"

    def execute_sql_query(self, sql_query: str) -> str:
        try:
            cursor = self.connection.cursor()

            sql_upper = sql_query.upper().strip()
            forbidden_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT', 'ALTER', 'CREATE', 'TRUNCATE']

            for keyword in forbidden_keywords:
                if keyword in sql_upper and f" {keyword} " in f" {sql_upper} ":
                    return f"Opération non autorisée: {keyword}. Seules les requêtes SELECT sont permises."

            cursor.execute(sql_query)

            if sql_upper.startswith('SELECT'):
                results = cursor.fetchall()
                if not results:
                    return "Requête exécutée - 0 résultat"

                columns = [description[0] for description in cursor.description]
                output = f"{len(results)} résultat(s) trouvé(s)\n\n"
                output += " | ".join(columns) + "\n"
                output += "-" * (len(" | ".join(columns)) + 10) + "\n"

                for row in results[:50]:  # Limite à 50 résultats pour éviter de surcharger
                    output += " | ".join(str(value) for value in row) + "\n"

                if len(results) > 10:
                    output += f"\n... et {len(results) - 10} résultat(s) supplémentaires"

                return output
            else:
                # Requête non-SELECT (mais autorisée comme PRAGMA)
                self.connection.commit()
                return "Requête exécutée avec succès"

        except sqlite3.Error as e:
            return f"Erreur SQL: {str(e)}"
        except Exception as e:
            return f"Erreur inattendue: {str(e)}"

    def explore_database(self, exploration_type: str = "overview") -> str:
        try:
            if exploration_type == "overview":
                return self.get_table_schema()
            elif exploration_type == "sample_data":
                cursor = self.connection.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()

                output = "Échantillon des données par table:\n\n"
                for table in tables:
                    table_name = table['name']
                    output += f"--- {table_name} ---\n"
                    cursor.execute(f"SELECT * FROM {quote_identifier(table_name)} LIMIT 3")
                    sample = cursor.fetchall()

                    if sample:
                        columns = [description[0] for description in cursor.description]
                        output += " | ".join(columns) + "\n"
                        for row in sample:
                            output += " | ".join(str(value) for value in row) + "\n"
                    output += "\n"

                return output
            else:
                return "Type d'exploration non reconnu"

        except Exception as e:
            return f"Erreur d'exploration: {str(e)}"

# Création de l'instance avec notre base exemple
sql_tool = GenericSQLTool("northwind.db")

# Création des outils LangChain
sql_tools = [
    Tool(
        name="SQL Schema Explorer",
        description="Explore le schéma de la base de données NORTHWIND. Input: 'overview' pour la liste des tables, ou un nom de table pour son schéma détaillé",
        func=sql_tool.get_table_schema
    ),
    Tool(
        name="SQL Query Executor",
        description="Exécute une requête SQL SELECT sur la base de données NORTHWIND. Input: la requête SQL complète. IMPORTANT: Ne peut exécuter que des SELECT pour des raisons de sécurité.",
        func=sql_tool.execute_sql_query
    ),
    Tool(
        name="Database Explorer",
        description="Explore le contenu de la base de données NORTHWIND. Input: 'overview' pour le schéma, 'sample_data' pour voir des échantillons de données",
        func=sql_tool.explore_database
    )
]

print("Outils SQL génériques créés!")

print("=== TEST SCHEMA EXPLORER ===")
print(sql_tool.get_table_schema())

print("\n=== TEST QUERY EXECUTOR ===")
print(sql_tool.execute_sql_query("SELECT * FROM Products WHERE UnitPrice > 1"))

print("\n=== TEST DATABASE EXPLORER ===")
print(sql_tool.explore_database("sample_data"))


# Attention trés dangereux !
class UnsafePythonTool:
    @staticmethod
    def execute_code(code: str) -> str:
        try:
            # Dictionnaires pour les variables globales et locales
            exec_globals = {}
            exec_locals = {}

            # Exécution du code arbitraire
            exec(code, exec_globals, exec_locals)

            # Retourner le résultat si 'result' est défini, sinon dernier contexte
            if "result" in exec_locals:
                return f"Résultat: {exec_locals['result']}"
            variables_info = "\n".join([f"{key}: {value}" for key, value in exec_locals.items()])

            return f"Code exécuté. Variables disponibles :\n{variables_info}"

        except Exception as e:
            return f"Erreur d'exécution: {str(e)}"


# Exemple d’utilisation
unsafe_python_tool = UnsafePythonTool()

python_tool = Tool(
    name="Python Executor",
    description="Exécute des expressions en Python. Input: expression Python simple comme '2 + 3 * 5'",
    func=unsafe_python_tool.execute_code
)

print(unsafe_python_tool.execute_code("result = (10 + 5) * 2"))
print(unsafe_python_tool.execute_code("""
x = 10
y = 20
result = x * y
"""))

from gtts import gTTS
from IPython.display import Audio
from langchain.tools import Tool

# Définition du tool TTS
class ColabTextToSpeechTool:
    @staticmethod
    def speak(text: str) -> Audio:
        try:
            tts = gTTS(text=text, lang='fr')  # Langue française par défaut
            tts.save("output.mp3")
            Audio("output.mp3", autoplay=True)
            return "audio created as output.mp3"
        except Exception as e:
            return f"Erreur TTS: {str(e)}"

# Créer l'outil LangChain
colab_tts_tool = Tool(
    name="Text-to-Speech",
    description="Convertit du texte en parole et enregistre le résultat en tant que audio.mp3 dans le répertoire courant.",
    func=ColabTextToSpeechTool.speak
)

# Exemple d’utilisation
ColabTextToSpeechTool.speak("Bonjour, ceci est un exemple de texte en parole.")


class AdvancedToolAgent:
    """Agent capable de coordonner plusieurs outils avancés"""

    def __init__(self):
        self.llm = llm

        # Combinaison de tous les outils
        self.all_tools = osm_tools + sql_tools + [python_tool] + web_tools + [colab_tts_tool]

        # Création de l'agent avec tous les outils
        self.agent = initialize_agent(
            tools=self.all_tools,
            llm=self.llm,
            agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=20
        )

    def solve_complex_problem(self, problem: str) -> str:
        """Résout un problème complexe en coordonnant les outils"""
        try:
            return self.agent.run(problem)
        except Exception as e:
            return f"Erreur lors de la résolution: {str(e)}"

# Création de l'agent avancé
advanced_agent = AdvancedToolAgent()
print("Agent avancé créé avec", len(advanced_agent.all_tools), "outils!")

TP8

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

# Chargement du PDF
loader = PyPDFLoader("notice_appareil.pdf")  # Ou utilisez votre propre PDF
documents = loader.load()

print(f"Document chargé : {len(documents)} pages")
print(f"Extrait de la première page : {documents[0].page_content[:500]}...")


# Configuration du splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,       # Taille maximale de chaque chunk
    chunk_overlap=200,     # Chevauchement entre les chunks
    length_function=len,
)

# Découpage des documents
texts = text_splitter.split_documents(documents)

print(f"Nombre de chunks créés : {len(texts)}")
print(f"Taille moyenne d'un chunk : {sum(len(text.page_content) for text in texts) / len(texts):.0f} caractères")
print(f"Exemple de chunk : {texts[0].page_content[:300]}...")

from langchain_google_genai import GoogleGenerativeAIEmbeddings
# from langchain_openai import OpenAIEmbeddings => si vous utilisez GPT

embeddings = GoogleGenerativeAIEmbeddings (model="models/gemini-embedding-001")
# embeddings = OpenAIEmbeddings () => si vous utilisez GPT

# Pour ce TP, utilisons OpenAI pour la performance
embeddings = embeddings
print("Modèle d'embeddings initialisé")

# Création du vecteur store
vector_store = Chroma.from_documents(
    documents=texts,
    embedding=embeddings,
    persist_directory="./chroma_db"  # Sauvegarde persistante
)

print(f"Base vectorielle créée avec {vector_store._collection.count()} embeddings")


# Test de recherche
query = "Comment allumer l'appareil ?"
results = vector_store.similarity_search(query, k=3)  # k = nombre de résultats

print(f"Résultats pour : '{query}'\n")
for i, doc in enumerate(results):
    print(f"Résultat {i+1}: {doc.page_content[:200]}...\n")


# Création de la chaine RAG
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # "stuff", "map_reduce", "refine"
    retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True,
    verbose=True
)

print("Chaine RAG créée avec succès")


# Test avec des questions
question = "Quel temps fait-il aujourd'hui ?"
result = qa_chain.invoke({"query": question})

print(f"Question : {question}")
print(f"Réponse : {result['result']}")
print(f"Documents sources utilisés : {len(result['source_documents'])}")

question = "Quelles sont les précautions de sécurité importantes ?"
result = qa_chain.invoke({"query": question})

print(f"Question : {question}")
print(f"Réponse : {result['result']}")
print(f"Documents sources utilisés : {len(result['source_documents'])}")


question = "How to set the alarm ?"
result = qa_chain.invoke({"query": question})

print(f"Question : {question}")
print(f"Réponse : {result['result']}")
print(f"Documents sources utilisés : {len(result['source_documents'])}")


TP9

import os
from google.colab import userdata
import gradio as gr
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
import tempfile
import shutil
from langchain_google_genai import GoogleGenerativeAIEmbeddings

class DocumentQASystem:
    """Système complet de Question/Réponse sur documents"""

    def __init__(self):
        self.vector_store = None
        self.qa_chain = None
        self.llm = llm
        self.embeddings = GoogleGenerativeAIEmbeddings (model="models/gemini-embedding-001")

    def process_document(self, file_path):
        """Charge et traite un document PDF"""
        try:
            # Chargement du PDF
            loader = PyPDFLoader(file_path)
            documents = loader.load()

            # Découpage en chunks
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            texts = text_splitter.split_documents(documents)

            # Création de la base vectorielle
            self.vector_store = Chroma.from_documents(
                documents=texts,
                embedding=self.embeddings
            )

            # Création de la chaine QA
            self.qa_chain = RetrievalQA.from_chain_type(
                llm=self.llm,
                chain_type="stuff",
                retriever=self.vector_store.as_retriever(search_kwargs={"k": 3}),
                return_source_documents=True
            )

            return f"Document traité avec succès! {len(texts)} chunks créés."

        except Exception as e:
            return f"Erreur lors du traitement: {str(e)}"

    def ask_question(self, question):
        """Pose une question sur le document chargé"""
        if not self.qa_chain:
            return "Veuillez d'abord charger un document."

        try:
            result = self.qa_chain.invoke({"query": question})
            return result["result"]
        except Exception as e:
            return f"Erreur: {str(e)}"

# Initialisation du système
qa_system = DocumentQASystem()


def create_interface():
    """Crée une interface Gradio avancée avec état"""

    def process_document(file_obj, history):
        """Traitement avec historique"""
        if file_obj is None:
            return "Sélectionnez un PDF", history

        history.append(("Système", f"Traitement de {file_obj.name}..."))

        try:
            # Traitement du fichier
            temp_dir = tempfile.mkdtemp()
            # Utilise uniquement le nom du fichier, pas le chemin complet
            file_path = os.path.join(temp_dir, os.path.basename(file_obj.name))
            shutil.copy(file_obj.name, file_path)

            result = qa_system.process_document(file_path)
            shutil.rmtree(temp_dir)

            history.append(("Système", result))
            return result, history, history


        except Exception as e:
            error_msg = f"Erreur: {str(e)}"
            history.append(("Système", error_msg))
            return error_msg, history

    def ask_question(question, history):
        """Pose une question avec historique"""
        if not question.strip():
            return "", history, history

        # Ajout de la question à l'historique
        history.append(("Utilisateur", question))

        # Obtention de la réponse
        answer = qa_system.ask_question(question)
        history.append(("Assistant", answer))

        return "", history, history

    def clear_conversation():
        """Réinitialise la conversation"""
        global qa_system
        qa_system = DocumentQASystem()
        return [], "Conversation réinitialisée!"

    # Interface avancée
    with gr.Blocks(title="Assistant Document IA", theme=gr.themes.Soft()) as interface:
        gr.Markdown("""
        # Assistant Document IA Avancé
        *Powered by LangChain & Gradio*
        """)

        # État de l'application
        chat_history = gr.State([])

        with gr.Row():
            with gr.Column(scale=1):
                gr.Markdown("### Document")
                file_input = gr.File(label="Uploader un PDF", file_types=[".pdf"])
                upload_btn = gr.Button("Traiter le document", variant="primary")
                upload_status = gr.Textbox(label="Status", interactive=False)
                clear_btn = gr.Button("Nouvelle conversation", variant="secondary")

            with gr.Column(scale=2):
                gr.Markdown("### Conversation")
                chatbot = gr.Chatbot(
                    label="Historique",
                    height=400,
                    show_copy_button=True
                )
                question_input = gr.Textbox(
                    label="Votre question",
                    placeholder="Posez une question sur le document...",
                    lines=2
                )
                ask_btn = gr.Button("Envoyer", variant="primary")

        # Interactions
        upload_btn.click(
            fn=process_document,
            inputs=[file_input, chat_history],
            outputs=[upload_status, chatbot, chat_history]
        )

        ask_btn.click(
            fn=ask_question,
            inputs=[question_input, chat_history],
            outputs=[question_input, chatbot, chat_history]
        )

        question_input.submit(
            fn=ask_question,
            inputs=[question_input, chat_history],
            outputs=[question_input, chatbot, chat_history]
        )

        clear_btn.click(
            fn=clear_conversation,
            outputs=[chatbot, upload_status]
        )

    return interface

# Création de l'interface
interface = create_interface()


def launch_application():
    """Lance l'application Gradio"""

    # Vous pouvez choisir l'interface simple ou avancée
    interface = create_interface()

    # Configuration du lancement
    interface.launch(
        share=True,  # Crée un lien public accessible
        debug=True,  # Mode debug pour le développement
        height=800   # Hauteur de la fenêtre
    )

interface.launch(share=True)

 